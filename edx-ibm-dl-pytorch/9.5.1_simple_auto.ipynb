{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://cocl.us/pytorch_link_top\">\n",
    "    <img src=\"https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0110EN/notebook_images%20/Pytochtop.png\" width=\"750\" alt=\"IBM Product \" />\n",
    "</a> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0110EN/notebook_images%20/cc-logo-square.png\" width=\"200\" alt=\"cognitiveclass.ai logo\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Autoencoders as Matrices </h1> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Table of Contents</h2>\n",
    "<p>In this lab, we will look at autoencoders as matrices. We will see how changing the shape in the shape of the latent space will changing the shape output. \n",
    "  </p>\n",
    "\n",
    "<ul>\n",
    "    <li><a href=\"#2D\">Autoencoders with 2D Latent Space as Matrice</a></li>\n",
    "    <li><a href=\"#1D\"> Autoencoders with 1D Latent Space as Matrices </a></li>\n",
    " \n",
    "</ul>\n",
    "\n",
    "<p>Estimated Time Needed: <strong>30 min</strong></p>\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll need the following libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are the libraries we are going to use in the lab.\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits import mplot3d\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 id=\"2D\">Autoencoders with 2D Latent Space = as Matrices</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create an Autoencoder custom module  or class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoEncoder(nn.Module): \n",
    "  \n",
    "    def __init__(self, input_dim=256, encoding_dim=32):\n",
    "        super(AutoEncoder, self).__init__()\n",
    "        \n",
    "        self.encoder = nn.Linear(input_dim,encoding_dim,bias=False)\n",
    "        self.decoder = nn.Linear(encoding_dim,input_dim,bias=False)\n",
    "    \n",
    "  \n",
    "    def forward(self, x):\n",
    " \n",
    "        x=self.encoder(x)\n",
    "        x=self.decoder(x)\n",
    "      \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We Create an Autoencoder object with a 2D input and 2D latent space as shown in the image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0110EN/DL0110EN/Version_3/Chapter_10/images/autencoderlinear2_2_2.png\" width=\"500\" alt=\"cognitiveclass.ai logo\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auto_encoder_2Dcode=AutoEncoder(2,2)\n",
    "auto_encoder_2Dcode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the weights are randomly initialized, we set them the orthogonal basis in the video for the encoder. As PyTorch treats the input as rows, we transpose all the wights.\n",
    "<p>\n",
    "   $\\quad\n",
    "    \\boldsymbol W= \\begin{pmatrix} \\frac{1}{\\sqrt{2}}& \\frac{1}{\\sqrt{2}} \\\\\n",
    "                             -\\frac{1}{\\sqrt{2}}  & \\frac{1}{\\sqrt{2}} \\end{pmatrix}  $ \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"encoder weight installation\", auto_encoder_2Dcode.state_dict()['encoder.weight'])\n",
    "\n",
    "W=torch.tensor([[1/2**(0.5),1/2**(0.5)],[-1/2**(0.5),1/2**(0.5)]])\n",
    "auto_encoder_2Dcode.state_dict()['encoder.weight'].data[:,:]=W\n",
    "print(\"new encoder weight \", auto_encoder_2Dcode.state_dict()['encoder.weight'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we will do the same for the decoder;\n",
    "<p>\n",
    "    <p>\n",
    "   $\\quad\n",
    "    \\boldsymbol W^T= \\begin{pmatrix} \\frac{1}{\\sqrt{2}}& -\\frac{1}{\\sqrt{2}} \\\\\n",
    "                             \\frac{1}{\\sqrt{2}}  & \\frac{1}{\\sqrt{2}} \\end{pmatrix}  $ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auto_encoder_2Dcode.state_dict()['decoder.weight'].data[:,:]=torch.transpose(W,0,1)\n",
    "auto_encoder_2Dcode.state_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can  get the encoder output  or code as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=torch.tensor([[1.0,1.0]])\n",
    "\n",
    "z=auto_encoder_2Dcode.encoder(x)\n",
    "z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can generate the outputs; it's identical to the input:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_hat=auto_encoder_2Dcode.decoder(z)\n",
    "x_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can produce the output by calling the forward function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_hat=auto_encoder_2Dcode(x)\n",
    "x_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can generate the code for multiple samples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=torch.tensor([[1.0,0],[0,1],[-1.0,0],[0,-1.0],[1,1],[-1,1],[1,-1],[-1,-1]])\n",
    "Z=auto_encoder_2Dcode.encoder(X)\n",
    "Z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see the output is the same as the code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xhat=auto_encoder_2Dcode(X)\n",
    "print('Xhat:')\n",
    "print(Xhat)\n",
    "print('X')\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see the output is the same as the code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xhat=auto_encoder_2Dcode(X)\n",
    "print('Xhat:')\n",
    "print(Xhat)\n",
    "print('X')\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following plot shows the input space and tensors or vectors on the left. The latent space and the code are on the right. Finally we have the code. The corresponding samples are/' colour coded accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors=['r','r','b','c','k','k','b','g','r'] \n",
    "\n",
    "for x,z,xhat,c in zip(X,Z,Xhat,colors):\n",
    "    plt.subplot(131)\n",
    "    \n",
    "    plt.quiver([0],[0],x[0].numpy(),x[1].numpy(),scale=5,color=c)\n",
    "    plt.title(' input space x') \n",
    "    plt.subplot(132)\n",
    "    plt.plot(z[0].detach().numpy(),z[1].detach().numpy(),c+'o')\n",
    "    plt.quiver([0],[0],0,1,scale=5,color='k')\n",
    "    plt.quiver([0],[0],1,0,scale=5,color='k')\n",
    "    plt.title('latent space z')\n",
    "    plt.subplot(133)\n",
    "    plt.quiver([0],[0],x[0].numpy(),x[1].numpy(),scale=5,color=c)\n",
    "\n",
    "    plt.title('output xhat')\n",
    "  \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 id=\"#1D\"> Autoencoders with 1D Latent Space as Matrices</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We Create an Autoencoder object with a 2D input and 1D latent space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0110EN/DL0110EN/Version_3/Chapter_10/images/under_complete.png\" width=\"500\" alt=\"cognitiveclass.ai logo\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auto_encoder_1Dcode=AutoEncoder(2,1)\n",
    "auto_encoder_1Dcode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can plot the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W=torch.tensor([[1/2**(0.5),1/2**(0.5)]])\n",
    "auto_encoder_2Dcode.state_dict()['encoder.weight'].data[:,:]=W\n",
    "\n",
    "auto_encoder_2Dcode.state_dict()['decoder.weight'].data[:,:]=torch.transpose(W,0,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z=auto_encoder_1Dcode.encoder(torch.tensor([[1.0,1.0]]))\n",
    "z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can generate the outputs; it's identical to the input:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_hat=auto_encoder_1Dcode.decoder(z)\n",
    "x_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can produce the output by calling the forward function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_hat=auto_encoder_1Dcode(x)\n",
    "x_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can generate the code for multiple samples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=torch.tensor([[1.0,0],[0,1],[-1,0],[0,-1],[1,1],[-1,1],[1,-1],[-1,-1]])\n",
    "Z=auto_encoder_1Dcode.encoder(X)\n",
    "Z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output is not the same, as there is not enough information to pass-through there encoder. As a result, all the output is vectors are scaler multiples of the vector $[1,1]$. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xhat=auto_encoder_1Dcode(X)\n",
    "print('Xhat:')\n",
    "print(Xhat)\n",
    "print('X')\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following plot shows the input space and tensors or vectors on the left. The latent space and the code are on the right. Finally we have the code each point vector is mapped to a point on a 1D line. Finally, we have  the output all the vectors span the line equivalent to $y=x$ or a scaler multiple of the vector $[1,1]$. The corresponding samples are/' colour coded accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors=['r','r','b','c','k','k','b','g','r'] \n",
    "\n",
    "for x,z,xhat,c in zip(X,Z,Xhat,colors):\n",
    "    plt.subplot(131)\n",
    "    \n",
    "    plt.quiver([0],[0],x[0].numpy(),x[1].numpy(),scale=5,color=c)\n",
    "    plt.title(' input space x') \n",
    "    plt.subplot(132)\n",
    "    plt.plot(z[0].detach().numpy(),0,c+'o')\n",
    "\n",
    "    plt.title('latent space z')\n",
    "    plt.subplot(133)\n",
    "    plt.quiver([0],[0],10*xhat[0].detach().numpy(),10*xhat[1].detach().numpy(),scale=5,color=c)\n",
    "\n",
    "    plt.title('output xhat')\n",
    "  \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--Empty Space for separating topics-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>About the Authors:</h2> \n",
    "\n",
    "<a href=\"https://www.linkedin.com/in/joseph-s-50398b136/\">Joseph Santarcangelo</a> has a PhD in Electrical Engineering, his research focused on using machine learning, signal processing, and computer vision to determine how videos impact human cognition. Joseph has been working for IBM since he completed his PhD."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright &copy; 2020 <a href=\"cognitiveclass.ai?utm_source=bducopyrightlink&utm_medium=dswb&utm_campaign=bdu\">cognitiveclass.ai</a>. This notebook and its source code are released under the terms of the <a href=\"https://bigdatauniversity.com/mit-license/\">MIT License</a>."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
